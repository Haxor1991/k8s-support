@include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
@include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
@include kubernetes.conf

<match **>
  @type google_cloud
  @id out_google_cloud

  use_metadata_service false

  project_id {{PROJECT_ID}}
  zone {{GCE_ZONE}}
  vm_id NODE_HOSTNAME
  vm_name NODE_HOSTNAME

  # Set the buffer type to file to improve the reliability and reduce the memory consumption
  buffer_type file
  buffer_path /var/log/fluentd-buffers/stackdriver.buffer
  # Set queue_full action to block because we want to pause gracefully
  # in case of the off-the-limits load instead of throwing an exception
  buffer_queue_full_action block
  # Set the chunk limit conservatively to avoid exceeding the GCL limit
  # of 10MiB per write request.
  buffer_chunk_limit 2M
  # Cap the combined memory usage of this buffer and the one below to
  # 2MiB/chunk * (6 + 2) chunks = 16 MiB
  buffer_queue_limit 6
  # Never wait more than 5 seconds before flushing logs in the non-error case.
  flush_interval 5s

  # When a buffer chunk fails to be flushed, retry after 20 seconds.
  # This delay is doubled on each following retry until either retry_limit or
  # max_retry_wait is reached.

  retry_wait 20s

  # Never wait longer than 300 seconds between retries.
  max_retry_wait 300

  # NOTE: we do not want to set disable_retry_limit as according to our current
  # understanding it would keep a problematic chunk in memory indefinitely,
  # leading to an increase in memory usage over time. Instead, we raised the
  # retry_limit from the default (3) to 10.
  # After 4 retries, every subsequent retry up to the tenth will happen after
  # a 300-second delay. This gives us more than half an hour to send a failed
  # chunk to Stackdriver again, which is likely more than enough to deal with
  # temporary network failures.

  retry_limit 10

  # Use multiple threads for processing.
  num_threads 2
</match>
