---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  # This is the CNI config needed for the cloud machines.
  cloud-cni-conf.json: |
    {
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  # This is the CNI config needed for the platform nodes.
  # This file should not contain any index_to_ip stuff. It is the backup config
  # for when multus isn't working or a pod is not tagged with any network
  # stuff.  Multus is supposed to preferentially get its config data out of
  # etcd, but it can't connect to etcd from non-Master nodes for reasons
  # unknown (likely related to it having a RFC1918 address for its
  # "ExternalIP").
  # Currently has no index_to_ip stuff in it to aid debugging.
  # TODO: Fix the ability of multus to connect to etcd so we can delete the
  # ipvlan stuff.
  platform-node-cni-conf.json: |
    {
     "name": "multus-network",
     "type": "multus",
     "delegates": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": false
          },
          "masterplugin": true
        },
        {
          "name": "ipvlan",
          "type": "ipvlan",
          "master": "eth0",
          "ipam": {
            "type": "host-local",
            "ranges": [
              [
              {
                "subnet": "4.14.159.112/26",
                "rangeStart": "4.14.159.119",
                "rangeEnd": "4.14.159.124",
                "gateway": "4.14.159.65"
              }
              ]
            ],
            "routes": [
              { "dst": "0.0.0.0/0" }
            ],
            "dataDir": "/tmp/host-local-state",
            "resolvConf": "/etc/resolv.conf"
          }
        }
      ]
    }
# This is what we think the config should be.
#    {
#      "name": "multus-network",
#      "type": "multus",
#      "delegates": [
#        {
#          "type": "flannel",
#          "delegate": {
#            "hairpinMode": true,
#            "isDefaultGateway": false
#          },
#          "masterplugin": true
#        }      ]
#    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
# Two daemonsets to set up the network. One for the platform nodes, and one for
# the cloud nodes. If a node has a value for mlab/type that is not platform or
# cloud or has no mlab/type annotation, then you are going to have a bad time.
# First is cloud nodes (check out the nodeSelector).
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-cloud
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
        mlab/type: cloud
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cloud-cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
# Next is platform nodes (check out the nodeSelector).
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-platform
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/arch: amd64
        mlab/type: platform
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/platform-node-cni-conf.json
        - /etc/cni/net.d/multus-cni.conf
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: true
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: "kubernetes.com/v1"
kind: Network
metadata:
 name: flannel-conf
plugin: flannel
args: '[ {
           "name": "flannel",
           "type": "flannel",
           "delegate": {
             "hairpinMode": true,
             "isDefaultGateway": false
           },
           "masterplugin": true
         } ]'
---
apiVersion: "kubernetes.com/v1"
kind: Network
metadata:
 name: index2ip-conf
# index_to_ip is a terrible terrible shell script. That is bad, and it should
# be replaced.  Follow along at http://github.com/m-lab/index2ip to watch its
# replacement come into being.
plugin: index_to_ip
args: '[ {
           "name": "ipvlan",
           "type": "ipvlan",
           "master": "eth0",
           "ipam": {
             "type": "index_to_ip"
           }
         } ]'
